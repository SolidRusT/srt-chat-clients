
# Vibe-Check Rubric for Language Models

## Purpose
The Vibe-Check Rubric is designed to evaluate and compare the performance of various language models (LMs) across a range of criteria. This tool aims to provide AI scientists and developers with a standardized framework to assess LMs on their understanding, creativity, accuracy, and more.

## Evaluation Criteria
The performance of language models will be evaluated based on the following criteria, each with a specified weight reflecting its importance:

1. **Language Understanding (40% Weight)**: Assesses the LM's comprehension and response accuracy to a wide range of input texts.
2. **Creativity (15% Weight)**: Evaluates the LM's ability to generate novel ideas, metaphors, and analogies.
3. **Factual Accuracy (20% Weight)**: Measures the LM's precision in recalling and conveying information across diverse domains.
4. **Context Sensitivity (10% Weight)**: Determines the LM's skill in adapting responses to match the context and tone of the conversation.
5. **Emotional Intelligence (10% Weight)**: Assesses the LM's capability to recognize and respond appropriately to emotional cues.
6. **Multi-Modal Communication (5% Weight)**: Evaluates the LM's ability to process and interpret non-verbal communication, such as images and audio.

## Scoring System
Each criterion will be scored on a scale from 1 to 10, with higher scores indicating better performance. The overall score will be calculated based on the weighted average of these scores.

## Implementation
Developers are encouraged to implement the evaluation mechanism as a software tool or platform that automates the assessment process. This includes:
- Input processing modules for handling various prompt types.
- Evaluation modules for scoring LM responses.
- A weighting system for calculating overall scores.

## Testing and Calibration
Initial manual evaluations of a set of LMs are recommended for calibrating the scoring system. Adjustments should be made based on these tests to ensure fairness and accuracy in evaluation.

## User Interface
A user-friendly interface should be designed to allow easy input of prompts, display the evaluation process, and show scoring breakdowns for each criterion.

## Feedback Loop
A mechanism for user feedback on criteria weights and scoring methods is crucial to ensure the rubric remains relevant and effective.

## Documentation
Comprehensive documentation and tutorials on using the app, including evaluated model examples and insights, will aid users in understanding and applying the evaluation framework.

---

This rubric is intended as a guide for AI scientists and developers working on language model evaluation and improvement. Feedback and contributions to refine and enhance this tool are welcome.
